# -*- coding: utf-8 -*-
"""Source.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RV0TctgCQRIphn3JHshZk4gBt60szV3q
"""

!pip install terminaltables
!pip install torchtext
!pip install transformers

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
import os
from IPython.display import HTML, display
from google.colab import files
import pandas as pd
import subprocess
import json
import numpy as np
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

auth.authenticate_user()
gauth =  GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""# Ego4D Moments Benchmark (NLQ) Quickstart

Please set your resources to GPU (Runtime -> Change runtime type -> GPU).

This quickstart will show:
1. An overview of the training data
2. How to train the baseline (VSLNet)

To begin: add your **access keys** below, change your Runtime Type to **GPU**, and run cells **one by one** as you read through. This helps avoid timeouts since Colab gives more GPU cycles to interactive notebooks.

## Resources
- [Baseline Repo](https://github.com/EGO4D/episodic-memory/tree/main/NLQ/VSLNet)
- [Docs](https://ego4d-data.org/docs/benchmarks/episodic-memory/)
- [EvalAI Challenge](https://eval.ai/web/challenges/challenge-page/1629/overview)

## Download Data and Setup Environment

### **Fill In Your Access Info Here**
If you don't have access and secret keys, first sign the Ego4D License at [ego4ddataset.com](https://ego4ddataset.com)
"""

os.environ['AWS_ACCESS_KEY_ID'] = " "
os.environ['AWS_SECRET_ACCESS_KEY'] = " "

"""### **Set up CLIs and Download Annotations + Repo**"""

# Commented out IPython magic to ensure Python compatibility.
# # Download the AWS and Ego4D CLIs, then download the annotations locally
# %%bash
# 
# # Set up the AWS CLI
# curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
# unzip -o awscliv2.zip >/dev/null
# sudo ./aws/install >/dev/null 2>&1
# aws configure set aws_access_key_id "$AWS_ACCESS_KEY_ID" && aws configure set aws_secret_access_key "$AWS_SECRET_ACCESS_KEY"
# rm "awscliv2.zip"

"""### Install the ego4d CLI and Download Data"""

# Set up the Ego4D CLI
!pip install ego4d

# Download the Ego4D Annotations to ego4d_data/
!ego4d --output_directory="/content/ego4d_data" --datasets annotations omnivore_video_swinl_fp16 --benchmarks nlq -y  --version v1

"""### Check Downloaded Files"""

# Ensure we have downloaded the files correctly
!ls /content/ego4d_data/v1/annotations | grep nlq

!ls /content/ego4d_data/v1/omnivore_video_swinl_fp16 | wc -l

"""### Clone the Episodic Memory Baseline Repository"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# git clone https://github.com/EGO4D/episodic-memory
# cd episodic-memory
# git pull
# git checkout nlq_fixes_and_fp16_support

"""# Stats for Data"""

# Commented out IPython magic to ensure Python compatibility.
import json
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

ann_data = json.load(open("/content/ego4d_data/v1/annotations/nlq_train.json"))

print(ann_data["version"])
print(ann_data["date"])
print(ann_data["description"])
print(ann_data["manifest"])
#print(ann_data["videos"])

# please see https://ego4d-data.org/docs/data/annotations-schemas/

anns = []
for vid in ann_data["videos"]:
    for clip in vid["clips"]:
        for ann in clip["annotations"]:
            for query in ann["language_queries"]:
                anns.append({
                    "id":vid["video_uid"],
                    "query_start_time_sec": clip["video_start_sec"],
                    "query_end_time_sec": clip["video_end_sec"],
                    "query_response_start_time_sec": query["video_start_sec"],
                    "query_response_end_time_sec": query["video_end_sec"],
                    "query_template": query.get("template", None),
                    "query": query.get("query", None),
                })

import pandas as pd
df = pd.DataFrame(anns)
fig = plt.figure()#figsize=(26, 55))
ax = fig.subplots()#13,1)
n=0
for group in df.groupby("query_template"):
  c=group[1]["query"].value_counts()
  temp = group[1]['query'].map(c)
  ax.plot(temp.value_counts().sort_index(),label=group[0])
  n+=1
fig.legend()

ann_data_ego4d = json.load(open("/content/ego4d_data/ego4d.json"))
anns_ego4d= []
for vid in ann_data_ego4d["videos"]:
    for item in vid["scenarios"]:
      if "Cooking" in item  or "commuting" in item:
        anns_ego4d.append({"id":vid["video_uid"]})

df = pd.DataFrame(anns_ego4d)
match_count = 0
unique_ann = set()
for item in anns:
  unique_ann.add(item["id"])
for item in anns_ego4d:
    if item['id'] in unique_ann:
        match_count += 1
print(f"Percentage of seek scenarios in dataset nlq_train : {match_count/len(unique_ann)*100:.2f}%")

print(anns[0])
print(anns[1])

num_queries = len(anns)
relative_query_sizes = np.array([
    (a["query_response_end_time_sec"] - a["query_response_start_time_sec"]) / (a["query_end_time_sec"] - a["query_start_time_sec"])
    for a in anns
])
query_sizes = np.array([
    (a["query_response_end_time_sec"] - a["query_response_start_time_sec"])
    for a in anns
])
clip_sizes = np.array([
    (a["query_end_time_sec"] - a["query_start_time_sec"])
    for a in anns
])

import numpy as np
print(len(query_sizes)-np.count_nonzero(query_sizes))

"""## Query / Response Durations

Here we can see that there are some queries with 0s. When training it is reccomended to remove them from the set. For VSLNet you can provide `--remove_empty_queries_from train` to `main.py` (e.g. if you are modifying this codebase).
"""

query_sizes.max(), query_sizes.min(), query_sizes.std()

clip_sizes.max(), clip_sizes.min(), clip_sizes.std()

# less than or equal to 4 frames => 9% of training data
(query_sizes <= 4/30).sum() / len(relative_query_sizes)

"""## Distribution of Queries (relative)

Here is a histogram plot of the relative query size to the clip size.
"""

plt.rcParams["figure.figsize"] = (16, 9)

plt.hist(relative_query_sizes[relative_query_sizes < 0.2], density=True, bins=128)
plt.show()

plt.hist(relative_query_sizes[relative_query_sizes > 0.2], density=True, bins=128)
plt.show()

"""## Clip Sizes

Clips are 522s on average, with most clips being 480s.
"""

clip_sizes.mean(), clip_sizes.max(), clip_sizes.min(), clip_sizes.std(), np.median(clip_sizes)

plt.hist(clip_sizes)
plt.show()

"""# Function used below

"""

def change_var_sh(file_path,var_name,new_value):
  import re
  # Leggi il contenuto del file
  with open(file_path, 'r') as file:
      lines = file.readlines()

  # Modifica il valore della variabile
  with open(file_path, 'w') as file:
      for line in lines:
          if line.startswith(f'export {var_name}='):
              # Trova la parte che corrisponde al valore attuale e sostituiscila
              new_line = re.sub(f'export {var_name}=.*', f'export {var_name}={new_value}', line)
              file.write(new_line + '\n')
          else:
              file.write(line)

def change_git(class_content,existing_file_path,replace):
  import re

  with open(existing_file_path, 'r') as f:
      existing_file_content = f.read()

  # Usa regex per trovare la definizione della classe e il suo contenuto
  pattern = re.compile(replace, re.DOTALL)

  new_file_content = re.sub(pattern, class_content, existing_file_content)

  # Scrivi il contenuto modificato nel file esistente
  with open(existing_file_path, 'w') as f:
      f.write(new_file_content)

def download_video(id):
    ##download the video associated to video/clip uid with cli command of EGO 4d.
    command = [
        'ego4d',
        '--output_directory=./data',
        '--datasets', 'clips',
        '--video_uids', id,
        '-y',
        '--version', 'v1'
    ]

    try:
      result = subprocess.run(command, check=True, text=True, capture_output=True,encoding='utf-8')
      print(result)
    except subprocess.CalledProcessError as e:
      print(e.stderr)

def cut_video(input_path, output_path, start_time, end_time):
    """This function uses ffmpeg to cut a specified segment from the input video file.
    It checks if the start_time or end_time are NaN values, and if so, skips the operation.
    If the times are valid, it constructs an ffmpeg command to extract the segment,
    executes the command, and handles any errors that may occur during the process.
    """
    if np.isnan(start_time) or np.isnan(end_time):
        print(f"Skipping video cut for {input_path} due to NaN values in start_time or end_time")
        return

    command = [
        'ffmpeg',
        '-i', input_path,
        '-ss', str(start_time),
        '-to', str(end_time),
        '-c', 'copy',# Copy codecs, avoid re-encoding
        '-y', #Overwrite the output file without asking for confirmation
        output_path
    ]

    try:
        print(f"Running command: {' '.join(command)}")
        result = subprocess.run(command, check=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)
        print(result.stdout.decode('utf-8'))
    except subprocess.CalledProcessError as e:
        print(f"Error occurred: {e.stderr.decode('utf-8')}")
        print(f"Command failed: {' '.join(command)}")

def download_mp4_files(directory):
    mp4_files = [f for f in os.listdir(directory) if f.endswith('.mp4')]

    for file in mp4_files:
        file_path = os.path.join(directory, file)
        files.download(file_path)
        print(f"Downloading {file_path}")

"""# VSL Net with Bert and features Omnivore

The NLQ baseline repository for VSLNet requires you to prepare the data for training and evaluation purposes. From the [README.md](https://github.com/EGO4D/episodic-memory/blob/main/NLQ/VSLNet/README.md#preparation) we need to run the `prepare_ego4d_dataset.py` script.

### Setup Environment Variables for NLQ

First let's setup some environment variables and setup the paths as NLQ's scripts will expect.
"""

with open("vars.sh", "w") as out_f:
  out_f.write("""
export NAME=VSLNet_omnivore_video_fp16
export TASK_NAME=nlq_official_v1_$NAME
export BASE_DIR=data/dataset/nlq_official_v1_$NAME
export FEATURE_BASE_DIR=data/features/nlq_official_v1_$NAME/
export FEATURE_DIR=$FEATURE_BASE_DIR/video_features
export MODEL_BASE_DIR=/content/nlq_official_v1/checkpoints/

cd episodic-memory/NLQ/VSLNet
"""
  )

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# echo $FEATURE_BASE_DIR
# mkdir -p $FEATURE_BASE_DIR
# ln -s /content/ego4d_data/v1/omnivore_video_swinl_fp16 $FEATURE_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# %%capture
# 
# source vars.sh
# pip install nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables

"""#### Code for extension

It is used to create a final file where the values ​​of the predictions and the corresponding evaluations are returned

"""

re = "average_IoU\s*=\s*\[\]"

new = "average_IoU = []\n    k={}"
path = "/content/episodic-memory/NLQ/VSLNet/utils/evaluate_ego4d_nlq.py"
change_git(new,path,re)
re = "num_instances\s*\+=\s*1"
new = "num_instances+=1\n        k[num_instances] = dict(clip_uid=pred_datum[\"clip_uid\"],IoU=overlap.tolist(),annotation_uid=pred_datum[\"annotation_uid\"],predicted_times=pred_datum[\"predicted_times\"])"
change_git(new,path,re)
re = "return\s+mean_results,\s+mIoU"
new = "return mean_results, mIoU , k"
change_git(new,path,re)
re = r"^import terminaltables$"
new="""import terminaltables\nimport torch\nimport random\nseed = 43\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.cuda.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False
"""
change_git(new,path,re)
re = r'results, mIoU = evaluate_nlq_performance\(\s*predictions\["results"\], ground_truth, args\["thresholds"\], args\["topK"\]\s*\)'

new="""results, mIoU ,k= evaluate_nlq_performance(
        predictions["results"], ground_truth, args["thresholds"], args["topK"]
    )\n    with open(\"temp.json\", \"w\") as file_id:\n          json.dump(k, file_id)"""
change_git(new,path,re)

re = "results,\s+mIoU\s*=\s*ego4d_eval\.evaluate_nlq_performance\("
new = "results, mIoU ,k= ego4d_eval.evaluate_nlq_performance("
path = "/content/episodic-memory/NLQ/VSLNet/utils/runner_utils.py"
change_git(new,path,re)

"""## Train and Evaluation VSL NET with omnivore

## Run the Prepare Script

This script will take a while to run and may not output progress until it is done. Please be patient.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# python utils/prepare_ego4d_dataset.py \
#     --input_train_split /content/ego4d_data/v1/annotations/nlq_train.json \
#     --input_val_split /content/ego4d_data/v1/annotations/nlq_val.json \
#     --input_test_split /content/ego4d_data/v1/annotations/nlq_test_unannotated.json \
#     --video_feature_read_path $FEATURE_DIR \
#     --clip_feature_save_path $FEATURE_BASE_DIR/official \
#     --output_save_path $BASE_DIR

"""## Train a Model

Please note:
1. These are *not* the parameters for the original baseline model in the Ego4D whitepaper.
2. Omnivore video features are used (slowfast was originally used), and their FP16 variant. This is due to free colab constraints (100GB disk space).

Omnivore video features do out-perform slowfast features.
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

"""You may have to re-run this cell after you run the training script. You can try to reload data."""

!mkdir -p /content/episodic-memory/NLQ/VSLNet/runs/

"""Unfortunately due to colab and the time taken in the below script (it first saves additional metadata to disk) - the cell below takes a while to get started. Please be patient when running it may take at least 30 minutes. You may get timed out from colab.

You can tell if the training is started by inspecting the filesystem on the left hand side. The directory: `episodic-memory/NLQ/VSLNet/runs` will populate with a subdirectory for the tensorboard logdir.

Please note, these are *not* the hyper parameters used for the baseline. The following uses an aggressive learning rate and a low number of epochs, to converge faster so you don't have to wait as long for this cell to finish. :)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=32
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# python main.py \
#     --task $TASK_NAME \
#     --predictor bert \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 1536 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train

"""# VSL Net whit GloVe and features Omnivore"""

import nltk
nltk.download('punkt')

!wget https://nlp.stanford.edu/data/glove.840B.300d.zip

!mkdir episodic-memory/NLQ/VSLNet/data
!mkdir episodic-memory/NLQ/VSLNet/data/features
!unzip glove.840B.300d.zip -d episodic-memory/NLQ/VSLNet/data/features/

"""## Run replacing Bert with GloVe in Omnivore"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=32
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# python main.py \
#     --task $TASK_NAME \
#     --predictor g \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 1536 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train

"""# VSL Base with Bert and features Omnivore"""

#changes to the existing vars.sh file
file_path = 'vars.sh'
var_name = 'NAME'
new_value = 'VSLBase_omnivore_video_fp16'
change_var_sh(file_path,var_name,new_value)

# override class in layer file to voncert from Vsl net to Vsl base
new_class_content="""class HighLightLayer(nn.Module):
      def __init__(self, dim):
          super(HighLightLayer, self).__init__()

      def forward(self, x, mask):
          return torch.ones(mask.shape[0], mask.shape[1]).cuda()

      @staticmethod
      def compute_loss(scores, labels, mask, epsilon=1e-12):
          return torch.tensor(0).cuda()
"""
change_git(new_class_content,'/content/episodic-memory/NLQ/VSLNet/model/layers.py',r'class HighLightLayer\(.*?\):.*?(?=class |\Z)')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# echo $FEATURE_BASE_DIR
# mkdir -p $FEATURE_BASE_DIR
# ln -s /content/ego4d_data/v1/omnivore_video_swinl_fp16 $FEATURE_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# %%capture
# 
# source vars.sh
# pip install nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables

"""## Train and Evaluation for VSL Base with omnivore

## Run the Prepare Script

This script will take a while to run and may not output progress until it is done. Please be patient.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# python utils/prepare_ego4d_dataset.py \
#     --input_train_split /content/ego4d_data/v1/annotations/nlq_train.json \
#     --input_val_split /content/ego4d_data/v1/annotations/nlq_val.json \
#     --input_test_split /content/ego4d_data/v1/annotations/nlq_test_unannotated.json \
#     --video_feature_read_path $FEATURE_DIR \
#     --clip_feature_save_path $FEATURE_BASE_DIR/official \
#     --output_save_path $BASE_DIR

"""## Train a Model"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=32
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# python main.py \
#     --task $TASK_NAME \
#     --predictor bert \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 1536 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train
#

# reset layer
prec_class_content="""class HighLightLayer(nn.Module):
    def __init__(self, dim):
        super(HighLightLayer, self).__init__()
        self.conv1d = Conv1D(
            in_dim=dim, out_dim=1, kernel_size=1, stride=1, padding=0, bias=True
        )

    def forward(self, x, mask):
        # compute logits
        logits = self.conv1d(x)
        logits = logits.squeeze(2)
        logits = mask_logits(logits, mask)
        # compute score
        scores = nn.Sigmoid()(logits)
        return scores

    @staticmethod
    def compute_loss(scores, labels, mask, epsilon=1e-12):
        labels = labels.type(torch.float32)
        weights = torch.where(labels == 0.0, labels + 1.0, 2.0 * labels)
        loss_per_location = nn.BCELoss(reduction="none")(scores, labels)
        loss_per_location = loss_per_location * weights
        mask = mask.type(torch.float32)
        loss = torch.sum(loss_per_location * mask) / (torch.sum(mask) + epsilon)
        return loss

"""

change_git(prec_class_content,'/content/episodic-memory/NLQ/VSLNet/model/layers.py',r'class HighLightLayer\(.*?\):.*?(?=class |\Z)')

"""# VSL Net with Bert and features Egovlp

"""

# ID file Google Drive (excerpt from shared link)
file_id = "1U318S34jw3uNnsURJ1T40YwsSuK5_-RJ"#1U318S34jw3uNnsURJ1T40YwsSuK5_-RJ"
file_name = 'egovlp_fp16.tar.gz'
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile(file_name)

# Commented out IPython magic to ensure Python compatibility.
# %mkdir -p /content/ego4d_data/v1/egovlp_fp16

# Extract the file
os.system(f'tar xf egovlp_fp16.tar.gz -C /content/ego4d_data/v1/egovlp_fp16')
# %rm /content/egovlp_fp16.tar.gz

#changes to the existing vars.sh file
file_path = 'vars.sh'

var_name = 'NAME'
new_value = 'VSLNet_egovlp_fp16'
change_var_sh(file_path,var_name,new_value)

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# echo $FEATURE_BASE_DIR
# mkdir -p $FEATURE_BASE_DIR
# ln -s /content/ego4d_data/v1/egovlp_fp16 $FEATURE_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# %%capture
# 
# source vars.sh
# pip install nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables

"""## Train and Evaluation for VSL NET with egovlp

### Run the Prepare Script

This script will take a while to run and may not output progress until it is done. Please be patient.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# python utils/prepare_ego4d_dataset.py \
#     --input_train_split /content/ego4d_data/v1/annotations/nlq_train.json \
#     --input_val_split /content/ego4d_data/v1/annotations/nlq_val.json \
#     --input_test_split /content/ego4d_data/v1/annotations/nlq_test_unannotated.json \
#     --video_feature_read_path $FEATURE_DIR \
#     --clip_feature_save_path $FEATURE_BASE_DIR/official \
#     --output_save_path $BASE_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# # Source the environment variables from vars.sh
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=64
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# # Run the Python script with the specified hyperparameters
# python main.py \
#     --task $TASK_NAME \
#     --predictor bert \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 256 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train

"""# VSL Net with GloVe and features Egovlp"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# # Source the environment variables from vars.sh
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=64
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# # Run the Python script with the specified hyperparameters
# python main.py \
#     --task $TASK_NAME \
#     --predictor g \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 256 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train

"""# VSL Base with Bert and features Egovlp"""

file_path = 'vars.sh'

var_name = 'NAME'
new_value = 'VSLBase_egovlp_fp16'
change_var_sh(file_path,var_name,new_value)
change_git(new_class_content,'/content/episodic-memory/NLQ/VSLNet/model/layers.py',r'class HighLightLayer\(.*?\):.*?(?=class |\Z)')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# echo $FEATURE_BASE_DIR
# mkdir -p $FEATURE_BASE_DIR
# ln -s /content/ego4d_data/v1/egovlp_fp16 $FEATURE_DIR

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# %%capture
# 
# source vars.sh
# pip install nltk submitit torch torchaudio torchvision tqdm transformers tensorboard Pillow terminaltables

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# python utils/prepare_ego4d_dataset.py \
#     --input_train_split /content/ego4d_data/v1/annotations/nlq_train.json \
#     --input_val_split /content/ego4d_data/v1/annotations/nlq_val.json \
#     --input_test_split /content/ego4d_data/v1/annotations/nlq_test_unannotated.json \
#     --video_feature_read_path $FEATURE_DIR \
#     --clip_feature_save_path $FEATURE_BASE_DIR/official \
#     --output_save_path $BASE_DIR

"""## Run VSL Base with egovlp"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# source vars.sh
# 
# export DATALOADER_WORKERS=1
# export NUM_WORKERS=2
# export VAL_JSON_PATH="/content/ego4d_data/v1/annotations/nlq_val.json"
# 
# export BATCH_SIZE=32
# export DIM=128
# export NUM_EPOCH=10
# export MAX_POS_LEN=128
# export INIT_LR=0.0025
# 
# export TB_LOG_NAME="${NAME}_bs${BATCH_SIZE}_dim${DIM}_epoch${NUM_EPOCH}_ilr${INIT_LR}"
# 
# python main.py \
#     --task $TASK_NAME \
#     --predictor bert \
#     --dim $DIM \
#     --mode train \
#     --video_feature_dim 256 \
#     --max_pos_len $MAX_POS_LEN \
#     --init_lr $INIT_LR \
#     --epochs $NUM_EPOCH \
#     --batch_size $BATCH_SIZE \
#     --fv official \
#     --num_workers $NUM_WORKERS \
#     --data_loader_workers $DATALOADER_WORKERS \
#     --model_dir $MODEL_BASE_DIR/$NAME \
#     --eval_gt_json $VAL_JSON_PATH \
#     --log_to_tensorboard $TB_LOG_NAME \
#     --tb_log_freq 5 \
#     --remove_empty_queries_from train

"""# Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/episodic-memory/NLQ/VSLNet/runs/

"""# Extension LLava"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 
# PRED_FILE="/content/nlq_official_v1/checkpoints/VSLNet_omnivore_video_fp16/vslnet_nlq_official_v1_VSLNet_omnivore_video_fp16_official_128_bert/model/vslnet_9_3230_preds.json"
# python /content/episodic-memory/NLQ/VSLNet/utils/evaluate_ego4d_nlq.py \
#     --ground_truth_json "/content/ego4d_data/v1/annotations/nlq_val.json" \
#     --model_prediction_json ${PRED_FILE} \
#     --thresholds 0.3 0.5 \
#     --topK 1 3 5

# fill the dataframe with the results of the validation
path = "/content/temp.json"
with open(path, 'r') as file:
    data = json.load(file)

df = pd.DataFrame([data.get(d) for d in data])
df["mIoU"]=df["IoU"].map(lambda x: np.mean(np.sort(x)[-3:]))

print(df.shape)
print(df.iloc[0],df.iloc[1],df.iloc[2])
print(df.columns)

"""## prepare the dataframe query"""

# Download of the annotation of the benchmark NLQ
os.environ["LC_ALL"] = "en_US.UTF-8"
os.environ["LANG"] = "en_US.UTF-8"
command = ["ego4d", "--output_directory=./ann", "--datasets", "annotations", "--benchmarks", "nlq", "-y", "--version", "v1"]

try:
    result = subprocess.run(command, check=True, text=True, capture_output=True, encoding='utf-8')
except subprocess.CalledProcessError as e:
    print(e)
    print(e.stdout)
    print(e.stderr)

# Save the real cuts for each clip query
path = "/content/ann/v1/annotations/nlq_val.json"

with open(path, 'r') as file:
    data = json.load(file)

video_data = []

for video in data['videos']:
    for clip in video["clips"]:
      #clip_uid
      rows = df[df['clip_uid'] == clip["clip_uid"]]
      if not rows.empty:
        for ann in clip["annotations"]:
          for q in ann["language_queries"]:
            m=ann["annotation_uid"]

            row = rows[rows['annotation_uid']== m]
            try :
              df.loc[row.index, 'query'] = q["query"]
            except KeyError:
              df.loc[row.index, 'query'] = None
            df.loc[row.index, 'clip_start_sec'] = q["clip_start_sec"]
            df.loc[row.index, 'clip_end_sec'] = q["clip_end_sec"]

from google.colab import files
csv_file_path = 'unique_values.csv'
df.to_csv(csv_file_path, index=False)
files.download(csv_file_path)

"""## Set LLva for our task

### set work enviroment and install specific library for extension
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# pip install torch==2.0.1 torchvision==0.15.2
# git clone https://github.com/PKU-YuanGroup/Video-LLaVA
# pip install einops
# pip install peft
# pip install pytorchvideo
# pip uninstall transformers -y
# pip install transformers
# 
# #pip install torch
# python -m pip install av
# cd Video-LLaVA
# conda create -n videollava python=3.10 -y
# conda activate videollava
# pip install --upgrade pip
# pip install -e .
# pip install -e ".[train]"
# pip install flash-attn --no-build-isolation
# pip install decord opencv-python git+https://github.com/facebookresearch/pytorchvideo.git@28fe037d212663c6a24f373b94cc5d478c8c1a1d
#

!mv /content/Video-LLaVA /content/Video_LLaVA

"""## Prompt  function for LLaVA"""

# Commented out IPython magic to ensure Python compatibility.
#!ls -all
# %cd Video-LLaVA
import os
import torch
from videollava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN
from videollava.conversation import conv_templates, SeparatorStyle
from videollava.model.builder import load_pretrained_model
from videollava.utils import disable_torch_init
from videollava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria
from decord import DECORDError
def pred(video,inp,tokenizer, model, processor,conv_mode): #conv_mode e model li eliminirei dagli input
     """
    Processes a video input and generates text predictions using a pre-trained model.

    Parameters:
    video (object): The video input to be processed.
    inp (str): The input text prompt for the model.
    tokenizer (Tokenizer): The tokenizer used to process the text input.
    model (PreTrainedModel): The pre-trained model used for generating predictions.
    processor (dict): A dictionary containing processors for different types of inputs, such as video.
    conv_mode (str): The conversation mode template to use.

    Returns:
    str: The generated text output from the model.

    This function performs the following steps:

    1. Processes the video input using the specified video processor.
    2. Converts the processed video tensor to the appropriate device and dtype.
    3. Prepares the input prompt by combining the input text with default image tokens.
    4. Tokenizes the input prompt.
    5. Sets up stopping criteria based on the conversation template.
    6. Generates text output using the pre-trained model.
    7. Decodes and returns the generated output.

    """
    os.chdir("/content/Video_LLaVA")


    '''prepare a structured prompt that combines images (represented by image tokens) and text in a format that the model can understand and use to generate a response.'''

    conv = conv_templates[conv_mode].copy() #copy the conversation template conv_mode is always setting as llva1
    roles = conv.roles # Defining the Roles in the Conversation
    #create the Input Prompt
    inp = ' '.join([DEFAULT_IMAGE_TOKEN] * model.get_video_tower().config.num_frames) + '\n' + inp
    #Adding Messages to the Conversation
    conv.append_message(conv.roles[0], inp) #input of user
    conv.append_message(conv.roles[1], None) #output of assistance is initially empty
    #generates the final conversation prompt
    prompt = conv.get_prompt()


    # PROCESS THE INPUT VIDEO
    video_processor = processor['video']
    video_tensor = video_processor(video, return_tensors='pt')['pixel_values']
    if type(video_tensor) is list:
        tensor = [video.to(model.device, dtype=torch.float16) for video in video_tensor]
    else:
        tensor = video_tensor.to(model.device, dtype=torch.float16)



    # TOKENIZE THE INPUT PROMPT
    '''converts the input prompt into
    a tensor of token IDs and prepares it for model input
    by adding a batch dimension and moving it to the GPU.'''
    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()

    # STOPPING CRITERIA
    '''sets up the criteria for when the model should stop generating text.'''
    stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
    keywords = [stop_str]
    stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)



    # GENERATE THE TEXT OUTPUT:
    '''The model generates text sequences in inference mode to optimize performance.
    It uses the processed input prompt and video frames, sampling parameters,
    and stopping criteria to produce the output tokens.'''
    with torch.inference_mode():
        output_ids = model.generate(
            input_ids,
            images=tensor,
            do_sample=True,
            temperature=0.1,
            max_new_tokens=1024,
            use_cache=True,
            stopping_criteria=[stopping_criteria])

    # DECODE AND RETURN THE OUTPUT
    '''The generated tokens are decoded into a readable string.'''
    outputs = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()
    return outputs

"""### LlaVA video question-answer task on first 50 queries"""

#path_out = "../clip"
#!ls -all
#%cd ..

#create the save directory
os.makedirs("/content/clip/", exist_ok=True)

# set the enviroment variable of LLva model
l=[] # empty for error
disable_torch_init()
model_path = 'LanguageBind/Video-LLaVA-7B'
cache_dir = 'cache_dir'
device = 'cuda'
load_4bit, load_8bit = True, False #load the model in  8bit version
model_name = get_model_name_from_path(model_path) #path of pretrained model

#load pretrain model
tokenizer, model, processor, _ = load_pretrained_model(model_path, None, model_name, load_8bit, load_4bit, device=device, cache_dir=cache_dir)
conv_mode = "llava_v1"


c=0
id=[]

#iterantion on first 50 row of dataframe sorted by descending mIou
for idx , row in df.sort_values(by='mIoU', ascending=False).iterrows():
    os.chdir("..") # set the directory to dowload the video
    download_video(row["clip_uid"])

    # cut the video according the predicted time segment
    '''cut the video using the first time span prediction for each query'''
    input_clip = f"/content/data/v1/clips/{row['clip_uid']}.mp4"
    cut_clip = f"/content/clip/{c}.mp4"

    cut_video(input_clip, cut_clip, row["predicted_times"][0][0],
              row["predicted_times"][0][1])

    df.loc[idx,"id"]=c
    inp = row["query"]

    try :
      answer = pred(cut_clip,inp,tokenizer, model, processor,conv_mode)
      df.loc[idx,"pred_LLaVA"] = out #update on dataframe the answer of model
      c+=1
      id.append(row['clip_uid'])
    # handle error of query not answered
    except DECORDError:
      l.append(c)
    if c==50:
      break

"""## Manual Annotation"""

# Download video for annotation
directory_path = '/content/clip'
download_mp4_files(directory_path)
# Download the data relating to the downloaded videos
temp=df.loc[:,["clip_uid","mIoU","pred_LLaVA","id"]].dropna().sort_values(by='mIoU', ascending=False)
print(temp)
csv_file_path = 'pred_LLaVA.csv'
temp.to_csv(csv_file_path, index=False)
files.download(csv_file_path)

#%cd ..
# Upload in a dataframe the data that contain the annotations
df3 = pd.read_csv('/content/pred_LLaVA_with_annotation.csv')

"""## Test LLva video answering task

The NLTK (Natural Language Toolkit) library is an open-source Python library that provides tools for working with natural language. It is primarily used in the field of natural language processing (NLP). NLTK is widely used for research, teaching, and developing applications that require the processing of natural language texts.
"""

# Evaluation between ground truth and the predictionof LLaVA
for idx , row in df3.iterrows():
  nltk.download('punkt')

  reference_text = row["ground_truth"]
  candidate_text = row["pred_LLaVA"]
  # tokenization of texts
  reference_tokens = nltk.word_tokenize(reference_text)
  candidate_tokens = nltk.word_tokenize(candidate_text)

  # Calculation of the BLEU score
  smoothie = SmoothingFunction().method4
  bleu_score = sentence_bleu([reference_tokens], candidate_tokens, smoothing_function=smoothie)
  print(f"BLEU score: {bleu_score}")
  df3.loc[idx,["BLEU"]] = bleu_score

"""## Viewing the highest scoring predictions"""

from IPython.display import Video
temp= df3.sort_values(by='BLEU', ascending=False).head(3)

t=temp.iloc[0]
video_path=f"/content/clip/{int(t['id'])}.mp4"
print(f"Score: {t['BLEU']}")
print(f"Query: {t['question']}")
print(f"Query: {t['pred_LLaVA']}")
Video(video_path, embed=True,width=400, height=250)

t=temp.iloc[1]
video_path=f"/content/clip/{int(t['id'])}.mp4"

print(f"Score: {t['BLEU']}")
print(f"Query: {t['question']}")
print(f"Query: {t['pred_LLaVA']}")
Video(video_path, embed=True,width=400, height=250)

t=temp.iloc[2]
video_path=f"/content/clip/{int(t['id'])}.mp4"
print(f"Score: {t['BLEU']}")
print(f"Query: {t['question']}")
print(f"Query: {t['pred_LLaVA']}")
Video(video_path, embed=True,width=400, height=250)