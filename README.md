# Project_Ego4D_NLQ_Benchmark
https://colab.research.google.com/drive/1RV0TctgCQRIphn3JHshZk4gBt60szV3q?usp=sharing

Project_Ego4D_NLQ_Benchmark This project explores the use of natural language queries for egocentric video understanding, leveraging the extensive Ego4D dataset and its NLQ benchmark. Our work is divided into two phases: video moment localization and the generation of textual answers based on the identified moments. In the first phase, we address the NLQ task by identifying the temporal segment of the video where a given natural language query is answerable. We model this as a regression problem using video span localizing networks: VSLBase and its enhanced version, VSLNet. Both networks learn cross-modal interactions between video and text features through an attention-like mechanism, and they are able to regress the span of the target moment. By incorporating the Query-Guided Highlighting (QGH) module, VSLNet improves performance by focusing the search of the target moment within the highlighted region. Additionally, we approach the task as a matching problem using a Temporal Adjacent Network (2D-TAN), which selects the optimal moment from a temporal 2D map. In the second phase, we extend the project by extracting textual answers from the identified target moments using a large vision-language model, Video-LLaVA. This project demonstrates progress towards developing an augmented reality assistant capable of interpreting daily-life egocentric videos and responding to queries about past activities.
